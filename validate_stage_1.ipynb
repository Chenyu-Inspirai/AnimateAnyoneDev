{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.pipeline_pose2img import Pose2ImagePipeline\n",
    "from src.models.pose_guider import PoseGuider\n",
    "from src.models.unet_2d_condition import UNet2DConditionModel\n",
    "from src.models.unet_3d import UNet3DConditionModel\n",
    "from src.pipelines.pipeline_pose2img import Pose2ImagePipeline\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from src.dwpose import DWposeDetector, draw_pose\n",
    "from controlnet_aux.util import HWC3, resize_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tools.openpose_rescaler import draw, rescale_skeleton, crop_and_resize\n",
    "import numpy as np\n",
    "import cv2\n",
    "from src.utils.util import get_fps, read_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dtype = torch.float32\n",
    "device = 'cuda:7'\n",
    "\n",
    "vae_model_path = '/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/pretrained_weights/sd-vae-ft-mse'\n",
    "\n",
    "base_model_path = '/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/pretrained_weights/sd-image-variations-diffusers'\n",
    "\n",
    "image_encoder_path = '/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/pretrained_weights/sd-image-variations-diffusers/image_encoder'\n",
    "\n",
    "config = OmegaConf.load('/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/configs/train/stage1.yaml')\n",
    "\n",
    "sched_kwargs = OmegaConf.to_container(config.noise_scheduler_kwargs)\n",
    "if config.enable_zero_snr:\n",
    "    sched_kwargs.update(\n",
    "        rescale_betas_zero_snr=True,\n",
    "        timestep_spacing=\"trailing\",\n",
    "        prediction_type=\"v_prediction\",\n",
    "    )\n",
    "    \n",
    "val_noise_scheduler = DDIMScheduler(**sched_kwargs)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(\n",
    "        device, dtype=weight_dtype\n",
    "    )\n",
    "\n",
    "reference_unet = UNet2DConditionModel.from_pretrained(\n",
    "    base_model_path,\n",
    "    subfolder=\"unet\",\n",
    ").to(device=device)\n",
    "\n",
    "denoising_unet = UNet3DConditionModel.from_pretrained_2d(\n",
    "    base_model_path,\n",
    "    \"\",\n",
    "    subfolder=\"unet\",\n",
    "    unet_additional_kwargs={\n",
    "        \"use_motion_module\": False,\n",
    "        \"unet_use_temporal_attention\": False,\n",
    "    },\n",
    ").to(device=device)\n",
    "\n",
    "image_enc = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    image_encoder_path,\n",
    ").to(dtype=weight_dtype, device=device)\n",
    "\n",
    "pose_guider = PoseGuider(\n",
    "            conditioning_embedding_channels=320, block_out_channels=(16, 32, 96, 256)\n",
    "        ).to(device=device)\n",
    "\n",
    "\n",
    "detector = DWposeDetector()\n",
    "detector = detector.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_net_weight_path = '/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/stage1_out/stage1_512_fintune_refnet_posguider_hdtf_tiktok/reference_unet-20000.pth'\n",
    "\n",
    "unet_weight_path = '/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/pretrained_weights/opensource_stage1/denoising_unet-0.pth'\n",
    "\n",
    "pose_guider_weight_path = '/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/stage1_out/stage1_512_fintune_refnet_posguider_hdtf_tiktok/pose_guider-20000.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising_unet.load_state_dict(torch.load(unet_weight_path, map_location=\"cpu\",),strict=False)\n",
    "\n",
    "reference_unet.load_state_dict(torch.load(ref_net_weight_path, map_location=\"cpu\"),strict=False)\n",
    "\n",
    "pose_guider.load_state_dict(torch.load(pose_guider_weight_path,map_location=\"cpu\"),strict=False)\n",
    "\n",
    "\n",
    "pipe = Pose2ImagePipeline(\n",
    "        vae=vae,\n",
    "        image_encoder=image_enc,\n",
    "        reference_unet=reference_unet,\n",
    "        denoising_unet=denoising_unet,\n",
    "        pose_guider=pose_guider,\n",
    "        scheduler=val_noise_scheduler,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image_file = '/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/assets/ref_image/anime_sample.jpeg'\n",
    "\n",
    "pose_image_file = '/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/configs/inference/ref_images/sample_00038_.png'\n",
    "\n",
    "pose_image_vid = '/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/assets/video_clips/dj.mp4'\n",
    "\n",
    "frame = 460\n",
    "\n",
    "W = 1024\n",
    "H = 1024\n",
    "\n",
    "ref_image = cv2.resize(np.array(Image.open(ref_image_file)), (W,H))\n",
    "\n",
    "first_frame = cv2.resize(np.array(read_frames(pose_image_vid)[0]), (W,H))\n",
    "\n",
    "ref_image = first_frame\n",
    "\n",
    "pose_image= Image.open(pose_image_file)\n",
    "\n",
    "pose_image_from_vid = read_frames(pose_image_vid)[frame]\n",
    "\n",
    "use_vid = 1\n",
    "\n",
    "ref_image = Image.fromarray(ref_image)\n",
    "\n",
    "if use_vid:\n",
    "    pose_image = pose_image_from_vid\n",
    "    \n",
    "pose_image = Image.fromarray(cv2.resize(np.array(pose_image), (W,H)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(30, 16))\n",
    "img_list = [ref_image, pose_image]\n",
    "\n",
    "for i, img in enumerate(img_list):\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print('extracting skeleton...')\n",
    "skeleton, score, ref_pose_keypoints = detector(pose_image, output_type='key_points', face=True)\n",
    "print('extraction done')\n",
    "# skeleton = Image.open('/cephfs/SZ-AI/usr/liuchenyu/HaiLook/Moore-AnimateAnyone/configs/inference/ref_images/openpose_2people.png')\n",
    "res_img = pipe(ref_image, skeleton, W, H, 25, 3.5).images\n",
    "res_img = res_img[0, :, 0].permute(1, 2, 0).cpu().numpy()  # (3, 512, 512)\n",
    "res_image_pil = Image.fromarray((res_img * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(30, 16))\n",
    "img_list = [ref_image, pose_image, skeleton, res_image_pil]\n",
    "\n",
    "for i, img in enumerate(img_list):\n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animateAnyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
